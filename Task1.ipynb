{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35709763-6a6c-4c28-9d80-b939c4b69bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Internship Task: Human Activity Recognition (HAR) using UCI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39c963-c2d4-42bd-966f-02441773147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Task 1: Train and Compare Classic ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf97e1f-ad7e-490e-9323-9b44eedec79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (7352, 561)\n",
      "Test data shape: (2947, 561)\n",
      "First few training labels:\n",
      "0    STANDING\n",
      "1    STANDING\n",
      "2    STANDING\n",
      "3    STANDING\n",
      "4    STANDING\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define file paths \n",
    "train_features_path = r\"C:\\Users\\vedant\\Downloads\\human+activity+recognition+using+smartphones\\UCI HAR Dataset\\UCI HAR Dataset\\train\\X_train.txt\"\n",
    "train_labels_path = r\"C:\\Users\\vedant\\Downloads\\human+activity+recognition+using+smartphones\\UCI HAR Dataset\\UCI HAR Dataset\\train\\y_train.txt\"\n",
    "test_features_path = r\"C:\\Users\\vedant\\Downloads\\human+activity+recognition+using+smartphones\\UCI HAR Dataset\\UCI HAR Dataset\\test\\X_test.txt\"\n",
    "test_labels_path = r\"C:\\Users\\vedant\\Downloads\\human+activity+recognition+using+smartphones\\UCI HAR Dataset\\UCI HAR Dataset\\test\\y_test.txt\"\n",
    "activity_labels_path = r\"C:\\Users\\vedant\\Downloads\\human+activity+recognition+using+smartphones\\UCI HAR Dataset\\UCI HAR Dataset\\activity_labels.txt\"\n",
    "\n",
    "# Load feature data \n",
    "X_train = pd.read_csv(train_features_path, delim_whitespace=True, header=None)\n",
    "X_test = pd.read_csv(test_features_path, delim_whitespace=True, header=None)\n",
    "\n",
    "# Load label data \n",
    "y_train = pd.read_csv(train_labels_path, delim_whitespace=True, header=None)\n",
    "y_test = pd.read_csv(test_labels_path, delim_whitespace=True, header=None)\n",
    "\n",
    "# Load activity labels\n",
    "activity_labels = pd.read_csv(activity_labels_path, delim_whitespace=True, header=None, index_col=0)\n",
    "\n",
    "# Map activity labels to their names\n",
    "y_train = y_train[0].map(activity_labels[1])\n",
    "y_test = y_test[0].map(activity_labels[1])\n",
    "\n",
    "# Display few data\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"First few training labels:\\n{y_train.head()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5f1653-56bc-4483-9f8d-b53b49dbc84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance:\n",
      "Accuracy: 0.9267\n",
      "Precision: 0.9278\n",
      "Recall: 0.9267\n",
      "F1 Score: 0.9266\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize the RandomForest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Training\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred_rf = rf_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
    "recall_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "# Print the results\n",
    "print(f\"Random Forest Performance:\")\n",
    "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
    "print(f\"Precision: {precision_rf:.4f}\")\n",
    "print(f\"Recall: {recall_rf:.4f}\")\n",
    "print(f\"F1 Score: {f1_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0af0285-aa55-4736-9124-1dd2d875bba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Performance:\n",
      "Accuracy: 0.8622\n",
      "Precision: 0.8633\n",
      "Recall: 0.8622\n",
      "F1 Score: 0.8617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Training\n",
    "dt_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred_dt = dt_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "precision_dt = precision_score(y_test, y_pred_dt, average='weighted')\n",
    "recall_dt = recall_score(y_test, y_pred_dt, average='weighted')\n",
    "f1_dt = f1_score(y_test, y_pred_dt, average='weighted')\n",
    "\n",
    "# Print the results\n",
    "print(f\"Decision Tree Performance:\")\n",
    "print(f\"Accuracy: {accuracy_dt:.4f}\")\n",
    "print(f\"Precision: {precision_dt:.4f}\")\n",
    "print(f\"Recall: {recall_dt:.4f}\")\n",
    "print(f\"F1 Score: {f1_dt:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5816450-23a6-45d6-a23c-db014d3093c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance:\n",
      "Accuracy: 0.9549\n",
      "Precision: 0.9567\n",
      "Recall: 0.9549\n",
      "F1 Score: 0.9548\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Training\n",
    "lr_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred_lr = lr_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "precision_lr = precision_score(y_test, y_pred_lr, average='weighted')\n",
    "recall_lr = recall_score(y_test, y_pred_lr, average='weighted')\n",
    "f1_lr = f1_score(y_test, y_pred_lr, average='weighted')\n",
    "\n",
    "# Print the results\n",
    "print(f\"Logistic Regression Performance:\")\n",
    "print(f\"Accuracy: {accuracy_lr:.4f}\")\n",
    "print(f\"Precision: {precision_lr:.4f}\")\n",
    "print(f\"Recall: {recall_lr:.4f}\")\n",
    "print(f\"F1 Score: {f1_lr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d6e627-6cad-4d1c-83aa-ff320407b182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Performance:\n",
      "Accuracy: 0.8778\n",
      "Precision: 0.8836\n",
      "Recall: 0.8778\n",
      "F1 Score: 0.8778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both the training and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the AdaBoost classifier with a base estimator \n",
    "ab_classifier = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=2), n_estimators=100, random_state=42)\n",
    "\n",
    "# Training\n",
    "ab_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred_ab = ab_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "accuracy_ab = accuracy_score(y_test, y_pred_ab)\n",
    "precision_ab = precision_score(y_test, y_pred_ab, average='weighted')\n",
    "recall_ab = recall_score(y_test, y_pred_ab, average='weighted')\n",
    "f1_ab = f1_score(y_test, y_pred_ab, average='weighted')\n",
    "\n",
    "# Print the results\n",
    "print(f\"AdaBoost Performance:\")\n",
    "print(f\"Accuracy: {accuracy_ab:.4f}\")\n",
    "print(f\"Precision: {precision_ab:.4f}\")\n",
    "print(f\"Recall: {recall_ab:.4f}\")\n",
    "print(f\"F1 Score: {f1_ab:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e1d370-d961-4b4f-9a31-cf52c263b2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0        Random Forest  0.926705   0.927771  0.926705  0.926584\n",
      "1        Decision Tree  0.862233   0.863273  0.862233  0.861652\n",
      "2  Logistic Regression  0.954869   0.956655  0.954869  0.954777\n",
      "3             AdaBoost  0.877842   0.883553  0.877842  0.877799\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "results = {\n",
    "    'Model': ['Random Forest', 'Decision Tree', 'Logistic Regression', 'AdaBoost'],\n",
    "    'Accuracy': [accuracy_rf, accuracy_dt, accuracy_lr, accuracy_ab],\n",
    "    'Precision': [precision_rf, precision_dt, precision_lr, precision_ab],\n",
    "    'Recall': [recall_rf, recall_dt, recall_lr, recall_ab],\n",
    "    'F1 Score': [f1_rf, f1_dt, f1_lr, f1_ab]\n",
    "}\n",
    "\n",
    "# DataFrame to display the results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Print the results\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63223a96-a5d1-4583-ad04-e0e370ef73cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Performance Summary\n",
    "\n",
    "In this task,classical ml models were evaluated on UCI HAR dataset.\n",
    "\n",
    "- Logistic Regression achieved the highest accuracy of 95.49%, along with the best precision,recall value and F1 Score.\n",
    "  \n",
    "- Random Forest followed closely with an accuracy of 92.67% though slightly less effective than Logistic Regression.\n",
    "\n",
    "- AdaBoost achieved accuracy of 87.78% along with decent precision,recall value and F1 Score.\n",
    "    \n",
    "- Decision Tree model had lowest accuracy rate of 86.22% along with lowest precision,recall value and F1 Score\n",
    "\n",
    "### Conclusion:\n",
    "Based on the results, Logistic Regression is the most effective model for this task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
